# text-to-SHACL

This repository contains the code for my Master's thesis **Text2SHACL: LLM-Driven Generation of Validation Graphs for Automatic Assessment of Social Benefit Eligibility**. The project explores how LLMs can support the automatic generation of SHACL shapes from natural language text.

TODO: A few more sentences on the thesis

## Author

**Name**: Seike Appold

**Email**: seike.appold@stud.leuphana.de

**Institution**: Leuphana University Lüneburg, Institute of Information Systems

**Program**: Management & Data Science

## An Example

TODO

## Usage

### Installation

Run `pip install -r requirements.txt`

### Runnning an Experiment

To run an experiment with the default configurations from the thesis, clone this repository and use the following command:

```bash
python Pipeline/Inference/RunInference.py   --mode <experiment_mode> \
                                            --api_key <your_api_key> \
                                            --base_url <your_base_url> \
```

Replace the parameters as follows:
- `<experiment_mode>`: One of "baseline", "fewshot", or "cot" (chain-of-thought).
- `<your_api_key>`: Your API key for the [Chat-AI API](https://docs.hpc.gwdg.de/services/saia/index.html#api-request).
- `<your_base_url>`: Base URL for the Chat-AI API endpoint.

Optionally, you can customize the following parameters:
- `num_examples`: Number of examples for fewshot or cot modes, defaults to 1.
- `k`: Number of folds for k-fold cross-validation in fewshot or cot modes, defaults to 3.
- `custom_models`: Space-separated list of model names to use.

### Evaluating an Experiment

To evaluate an experiment run, use the following command:
```bash 
# TODO: Insert code
```

### Implementation Details

The experiments and evaluation were conducted using `Python 3.12.3` on `Ubuntu 24.04.2 LTS`, with model inference run on the high-performance computing system provided by the [Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen](https://docs.hpc.gwdg.de/services/chat-ai/).

## Data Source

The dataset consists of two parts:

- **Requirements texts**: The natural language descriptions of eligibility requirements for social benefits were retrieved using the [Suchdienst-API](https://anbindung.pvog.cloud-bdc.dataport.de/api/suchdienst) from the [Portalverbund Online Gateway (PVOG)](https://anbindung.pvog.cloud-bdc.dataport.de/about-us). The data was downloaded on **January 20, 2025**. For the raw data, only exemplary files are included on remote.

- **SHACL Gold**: The shapes graphs used as ground truth in this project were generated by the author, and independently verified by two domain experts.

## File Structure

The repository is structured as follows.

- text-to-SHACL
    - **[data](data)**
        - **[processed](data/processed)**
            - **[requirements_texts](data/processed/shacl_gold)**: Input requirements texts for selected benefits.
            - **[shacl_gold](data/processed/requirements_texts)**: Human-annotated SHACL graphs (ground truth).
        - **[raw](data/raw)**
            - **[service_catalogs](data/raw/service_catalogs)**: All administrative services by municiaplity.
            - **[all_service_descriptions](data/raw/service_descriptions)**: Full descriptions of all adminsitrative services.
            - **[social_benefit_descriptions](data/raw/social_benefit_descriptions)**: Intermediate benefit selection.
    - **[Pipeline](Pipeline)**
        - **[Inference](Pipeline/Inference)**: Code for generating model output with different prompts.
        - **[Evaluation](Pipeline/Evaluation)**: Code for evaluating LLM-generated SHACL shapes.
    - **[Plotting](Plotting)**: Code and files visualizing the evaluation results.
    - **[Preprocessing](Preprocessing)**: Code for scraping and preparing the dataset.
    - **[resources](resources)**
        - **[requiremets_decomposition](resources/requirements_decomposition)**: Extracted individual requirements.
        - **[schemata](resources/schemata)**: Metadata about benefits and experiments.
        - **[templates](resources/templates)**: SHACL & Decomposition templates.
        - **[user_profiles](resources/user_profiles)**: Synthetic usre profiles in RDF.
    - **[results](path/)**: Model output and evaluation metrics per experiment run.
         - <run_id>/: One folder per experiment run (e.g., baseline_0ex0fcv_1745664019)
            - <model_name>/: One folder per model used in the run
                - metrics/: Evaluation metrics for this model
                - output/: Raw and parsed model outputs
                    - parsed_output/: Generated SHACL graphs per requirement
    - **[Utils](Utils)**: General helper functions used throughout the repository.


## Acknowledgments

Special thanks to Ben and Benjamin from [Förderfunke](https://foerderfunke.org/) for inspiring the use of SHACL and RDF for eligibility assessment and supporting the annotation process with their practical expertise. For those curious about civic tech their [GitHub](https://github.com/Citizen-Knowledge-Graph) and website linked above are well worth exploring.